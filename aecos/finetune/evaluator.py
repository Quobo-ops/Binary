"""ModelEvaluator — benchmarks against golden test set."""

from __future__ import annotations

import json
import logging
from dataclasses import dataclass, field
from typing import Any

from aecos.finetune.golden_set import GOLDEN_TEST_SET
from aecos.nlp.providers.base import LLMProvider

logger = logging.getLogger(__name__)


@dataclass
class EvaluationReport:
    """Report from model evaluation against the golden test set."""

    intent_accuracy: float = 0.0
    ifc_class_accuracy: float = 0.0
    property_accuracy: float = 0.0
    material_accuracy: float = 0.0
    overall_score: float = 0.0
    total_cases: int = 0
    results: list[dict[str, Any]] = field(default_factory=list)

    def to_dict(self) -> dict[str, Any]:
        return {
            "intent_accuracy": self.intent_accuracy,
            "ifc_class_accuracy": self.ifc_class_accuracy,
            "property_accuracy": self.property_accuracy,
            "material_accuracy": self.material_accuracy,
            "overall_score": self.overall_score,
            "total_cases": self.total_cases,
        }

    def to_markdown(self) -> str:
        """Generate a Markdown evaluation report."""
        lines = [
            "# Model Evaluation Report",
            "",
            "## Scores",
            "",
            f"| Metric | Score |",
            f"|--------|-------|",
            f"| Intent Accuracy | {self.intent_accuracy:.1%} |",
            f"| IFC Class Accuracy | {self.ifc_class_accuracy:.1%} |",
            f"| Property Accuracy | {self.property_accuracy:.1%} |",
            f"| Material Accuracy | {self.material_accuracy:.1%} |",
            f"| **Overall Score** | **{self.overall_score:.1%}** |",
            "",
            f"Total test cases: {self.total_cases}",
            "",
            "---",
            "*Generated by AEC OS Model Evaluator*",
            "",
        ]
        return "\n".join(lines)


class ModelEvaluator:
    """Evaluate an NLP provider against the golden test set.

    Parameters
    ----------
    golden_set:
        Optional custom golden set. Defaults to the embedded set.
    """

    def __init__(
        self,
        golden_set: list[dict[str, Any]] | None = None,
    ) -> None:
        self._golden_set = golden_set or GOLDEN_TEST_SET

    def evaluate(self, provider: LLMProvider) -> EvaluationReport:
        """Run the golden test set through the provider and score results.

        Parameters
        ----------
        provider:
            The LLM provider to evaluate (must have a ``parse`` method
            or be usable via FallbackProvider).

        Returns
        -------
        EvaluationReport
        """
        results: list[dict[str, Any]] = []
        intent_correct = 0
        ifc_correct = 0
        prop_scores: list[float] = []
        mat_scores: list[float] = []

        for case in self._golden_set:
            prompt = case["prompt"]
            expected = case["expected"]

            try:
                spec = provider.parse(prompt, None)
                actual = {
                    "intent": spec.intent,
                    "ifc_class": spec.ifc_class,
                    "properties": spec.properties,
                    "materials": spec.materials,
                    "performance": spec.performance,
                    "constraints": spec.constraints,
                    "compliance_codes": spec.compliance_codes,
                }
            except Exception as exc:
                logger.warning("Evaluation failed for '%s': %s", prompt[:50], exc)
                actual = {}

            # Score intent
            if actual.get("intent") == expected.get("intent"):
                intent_correct += 1

            # Score IFC class
            if actual.get("ifc_class") == expected.get("ifc_class"):
                ifc_correct += 1

            # Score properties (partial match)
            prop_score = _score_properties(
                actual.get("properties", {}),
                expected.get("properties", {}),
            )
            prop_scores.append(prop_score)

            # Score materials (overlap)
            mat_score = _score_materials(
                actual.get("materials", []),
                expected.get("materials", []),
            )
            mat_scores.append(mat_score)

            results.append({
                "prompt": prompt,
                "expected": expected,
                "actual": actual,
                "intent_match": actual.get("intent") == expected.get("intent"),
                "ifc_match": actual.get("ifc_class") == expected.get("ifc_class"),
                "property_score": prop_score,
                "material_score": mat_score,
            })

        total = len(self._golden_set) or 1

        intent_acc = intent_correct / total
        ifc_acc = ifc_correct / total
        prop_acc = sum(prop_scores) / total if prop_scores else 0.0
        mat_acc = sum(mat_scores) / total if mat_scores else 0.0

        overall = (intent_acc + ifc_acc + prop_acc + mat_acc) / 4

        return EvaluationReport(
            intent_accuracy=intent_acc,
            ifc_class_accuracy=ifc_acc,
            property_accuracy=prop_acc,
            material_accuracy=mat_acc,
            overall_score=overall,
            total_cases=total,
            results=results,
        )


def _score_properties(
    actual: dict[str, Any], expected: dict[str, Any],
) -> float:
    """Score property match (0.0-1.0)."""
    if not expected:
        return 1.0  # No properties expected — auto-pass

    if not actual:
        return 0.0

    matches = 0
    total = len(expected)

    for key, exp_val in expected.items():
        act_val = actual.get(key)
        if act_val is not None:
            # Allow 5% tolerance for numeric values
            if isinstance(exp_val, (int, float)) and isinstance(act_val, (int, float)):
                if abs(exp_val - act_val) <= abs(exp_val) * 0.05 + 0.01:
                    matches += 1
            elif act_val == exp_val:
                matches += 1

    return matches / total if total > 0 else 1.0


def _score_materials(
    actual: list[str], expected: list[str],
) -> float:
    """Score material match (0.0-1.0) based on overlap."""
    if not expected:
        return 1.0

    if not actual:
        return 0.0

    actual_lower = {m.lower() for m in actual}
    expected_lower = {m.lower() for m in expected}

    overlap = actual_lower & expected_lower
    return len(overlap) / len(expected_lower) if expected_lower else 1.0
